"""
Entry service for managing journal entries.
"""
import uuid
from datetime import date, datetime
from typing import List, Optional

from sqlalchemy.exc import SQLAlchemyError
from sqlmodel import Session, select
from zoneinfo import ZoneInfo

from app.core.exceptions import EntryNotFoundError, JournalNotFoundError, ValidationError
from app.core.logging_config import log_debug, log_info, log_warning, log_error
from app.core.time_utils import utc_now, local_date_for_user, ensure_utc, to_utc
from app.models.entry import Entry, EntryMedia
from app.models.entry_tag_link import EntryTagLink
from app.models.integration import IntegrationProvider
from app.models.journal import Journal
from app.schemas.entry import EntryCreate, EntryUpdate, EntryMediaCreate, EntryMediaCreateRequest
from app.utils.quill_delta import extract_plain_text, extract_media_sources

DEFAULT_ENTRY_PAGE_LIMIT = 50
MAX_ENTRY_PAGE_LIMIT = 100


class EntryService:
    """Service class for entry operations."""

    def __init__(self, session: Session):
        self.session = session

    @staticmethod
    def _normalize_limit(limit: int) -> int:
        """Normalize pagination limit to valid range."""
        if limit <= 0:
            return DEFAULT_ENTRY_PAGE_LIMIT
        return min(limit, MAX_ENTRY_PAGE_LIMIT)

    @staticmethod
    def _escape_like_pattern(query: str) -> str:
        """Escape SQL LIKE wildcards (% and _) in user query."""
        if not query:
            return query
        return query.replace("\\", "\\\\").replace("%", "\\%").replace("_", "\\_")

    def _get_owned_entry(self, entry_id: uuid.UUID, user_id: uuid.UUID, *, include_deleted: bool = False) -> Entry:
        statement = select(Entry).where(
            Entry.id == entry_id,
            Entry.user_id == user_id,
        )

        entry = self.session.exec(statement).first()
        if not entry:
            log_warning(f"Entry not found for user {user_id}: {entry_id}")
            raise EntryNotFoundError("Entry not found")
        return entry

    def _commit(self) -> None:
        """Commit database changes with proper error handling."""
        try:
            self.session.commit()
        except SQLAlchemyError as exc:
            self.session.rollback()
            log_error(exc)
            raise

    @staticmethod
    def _derive_entry_date(entry_datetime_utc: datetime, timezone_name: str) -> date:
        """Determine the local date for an entry based on stored timezone."""
        return local_date_for_user(entry_datetime_utc, timezone_name or "UTC")

    def _normalize_entry_timestamp(
        self,
        *,
        entry_date: Optional[date],
        entry_datetime_utc: Optional[datetime],
        entry_timezone: Optional[str],
        fallback_timezone: str
    ) -> tuple[datetime, str, date]:
        timezone_name = (entry_timezone or fallback_timezone or "UTC").strip() or "UTC"

        if entry_datetime_utc is not None:
            normalized_dt = ensure_utc(entry_datetime_utc)
        elif entry_date is not None:
            local_now = datetime.now(ZoneInfo(timezone_name))
            local_dt = datetime.combine(entry_date, local_now.time())
            normalized_dt = to_utc(local_dt, timezone_name)
        else:
            normalized_dt = utc_now()

        derived_date = self._derive_entry_date(normalized_dt, timezone_name)
        return normalized_dt, timezone_name, derived_date

    def _refresh_entry_date(self, entry: Entry) -> None:
        utc_dt = ensure_utc(entry.entry_datetime_utc)
        entry.entry_date = self._derive_entry_date(utc_dt, entry.entry_timezone)

    def create_entry(
        self,
        user_id: uuid.UUID,
        entry_data: EntryCreate,
        *,
        is_draft: bool = False,
    ) -> Entry:
        """Create a new entry in a journal.

        Args:
            user_id: User ID creating the entry
            entry_data: Entry creation data

        Returns:
            Created entry instance
        """
        # Validate journal exists and belongs to user
        journal_statement = select(Journal).where(
            Journal.id == entry_data.journal_id,
            Journal.user_id == user_id
        )
        journal = self.session.exec(journal_statement).first()
        if not journal:
            log_warning(f"Journal not found for user {user_id}: {entry_data.journal_id}")
            raise JournalNotFoundError("Journal not found")

        if entry_data.content_delta is not None:
            delta_payload = entry_data.content_delta.model_dump()
            sources = extract_media_sources(delta_payload)
            log_debug(
                "Entry create: incoming delta media sources",
                user_id=user_id,
                journal_id=str(entry_data.journal_id),
                media_source_count=len(sources),
                redacted_media_ids=[f"{s[:8]}..." for s in sources[:5]],
            )

        plain_text = extract_plain_text(
            entry_data.content_delta.model_dump() if entry_data.content_delta else None
        )
        word_count = len(plain_text.split()) if plain_text else 0

        from app.services.user_service import UserService
        user_service = UserService(self.session)
        user_tz = user_service.get_user_timezone(user_id)

        entry_dt_utc, entry_tz, entry_date = self._normalize_entry_timestamp(
            entry_date=entry_data.entry_date,
            entry_datetime_utc=entry_data.entry_datetime_utc,
            entry_timezone=entry_data.entry_timezone,
            fallback_timezone=user_tz
        )

        entry = Entry(
            title=entry_data.title,
            content_delta=entry_data.content_delta.model_dump() if entry_data.content_delta else None,
            content_plain_text=plain_text or None,
            entry_date=entry_date,
            entry_datetime_utc=entry_dt_utc,
            entry_timezone=entry_tz,
            journal_id=entry_data.journal_id,
            prompt_id=entry_data.prompt_id,
            word_count=word_count,
            user_id=user_id,
            is_draft=is_draft,
            # Structured location/weather fields
            location_json=entry_data.location_json,
            latitude=entry_data.latitude,
            longitude=entry_data.longitude,
            weather_json=entry_data.weather_json,
            weather_summary=entry_data.weather_summary,
        )

        try:
            self.session.add(entry)
            self._commit()
            self.session.refresh(entry)
        except SQLAlchemyError as exc:
            self.session.rollback()
            log_error(exc)
            raise

        log_info(
            f"Entry created for user {user_id} in journal {entry.journal_id}: {entry.id} (draft={is_draft})"
        )

        if not is_draft:
            try:
                from app.services.journal_service import JournalService
                JournalService(self.session).recalculate_journal_entry_count(entry.journal_id, user_id)
            except JournalNotFoundError:
                log_warning(f"Journal missing during entry recount for user {user_id}: {entry.journal_id}")
            except SQLAlchemyError as exc:
                log_error(exc)
            except Exception as exc:
                log_error(exc)

            # Update writing streak analytics
            try:
                from app.services.analytics_service import AnalyticsService
                analytics_service = AnalyticsService(self.session)
                analytics_service.update_writing_streak(user_id, entry.entry_date)
            except Exception as exc:
                log_error(exc)

        return entry

    def finalize_entry(self, entry_id: uuid.UUID, user_id: uuid.UUID) -> Entry:
        """Finalize a draft entry."""
        entry = self._get_owned_entry(entry_id, user_id)

        if not entry.is_draft:
            return entry

        entry.is_draft = False
        entry.updated_at = utc_now()
        self._refresh_entry_date(entry)
        plain_text = extract_plain_text(entry.content_delta)
        entry.content_plain_text = plain_text or None
        entry.word_count = len(plain_text.split()) if plain_text else 0

        try:
            self.session.add(entry)
            self._commit()
            self.session.refresh(entry)
        except SQLAlchemyError as exc:
            self.session.rollback()
            log_error(exc)
            raise

        try:
            from app.services.journal_service import JournalService
            JournalService(self.session).recalculate_journal_entry_count(entry.journal_id, user_id)
        except JournalNotFoundError:
            log_warning(f"Journal missing during entry recount for user {user_id}: {entry.journal_id}")
        except SQLAlchemyError as exc:
            log_error(exc)
        except Exception as exc:
            log_error(exc)

        try:
            from app.services.analytics_service import AnalyticsService
            analytics_service = AnalyticsService(self.session)
            analytics_service.update_writing_streak(user_id, entry.entry_date)
        except Exception as exc:
            log_error(exc)

        log_info(f"Entry finalized for user {user_id}: {entry.id}")
        return entry

    def get_entry_by_id(self, entry_id: uuid.UUID, user_id: uuid.UUID) -> Optional[Entry]:
        """Get an entry by ID, ensuring it belongs to the user."""
        statement = select(Entry).where(
            Entry.id == entry_id,
            Entry.user_id == user_id,
        )
        return self.session.exec(statement).first()

    def get_journal_entries(
        self,
        journal_id: uuid.UUID,
        user_id: uuid.UUID,
        limit: int = DEFAULT_ENTRY_PAGE_LIMIT,
        offset: int = 0,
        include_pinned: bool = True,
        include_drafts: bool = False,
        hydrate_media: bool = True,
    ) -> List[Entry]:
        """Get entries for a specific journal.

        The media_count field is automatically synchronized by database triggers.
        """
        from app.services.journal_service import JournalService
        JournalService(self.session)._get_owned_journal(journal_id, user_id)

        statement = select(Entry).where(Entry.journal_id == journal_id)

        if not include_drafts:
            statement = statement.where(Entry.is_draft.is_(False))

        if not include_pinned:
            statement = statement.where(Entry.is_pinned.is_(False))

        statement = statement.order_by(
            Entry.is_pinned.desc(),
            Entry.entry_datetime_utc.desc()
        ).offset(offset).limit(limit)

        entries = list(self.session.exec(statement))
        if hydrate_media:
            return [self._hydrate_entry(entry, user_id) for entry in entries]
        return entries

    def get_user_entries(
        self,
        user_id: uuid.UUID,
        limit: int = DEFAULT_ENTRY_PAGE_LIMIT,
        offset: int = 0,
        hydrate_media: bool = True,
        include_drafts: bool = False,
    ) -> List[Entry]:
        """Get all entries for a user across all journals, sorted by entry_datetime_utc descending."""
        statement = select(Entry).where(
            Entry.user_id == user_id,
        ).order_by(Entry.entry_datetime_utc.desc())

        if not include_drafts:
            statement = statement.where(Entry.is_draft.is_(False))

        statement = statement.offset(offset).limit(limit)

        entries = list(self.session.exec(statement))
        if hydrate_media:
            return [self._hydrate_entry(entry, user_id) for entry in entries]
        return entries

    def get_user_drafts(
        self,
        user_id: uuid.UUID,
        limit: int = DEFAULT_ENTRY_PAGE_LIMIT,
        offset: int = 0,
        journal_id: Optional[uuid.UUID] = None,
        hydrate_media: bool = True,
    ) -> List[Entry]:
        """Get all draft entries for a user, newest updated first."""
        statement = select(Entry).where(
            Entry.user_id == user_id,
            Entry.is_draft.is_(True),
        )

        if journal_id:
            statement = statement.where(Entry.journal_id == journal_id)

        statement = statement.order_by(
            Entry.updated_at.desc(),
            Entry.entry_datetime_utc.desc(),
        ).offset(offset).limit(limit)

        entries = list(self.session.exec(statement))
        if hydrate_media:
            return [self._hydrate_entry(entry, user_id) for entry in entries]
        return entries

    def update_entry(self, entry_id: uuid.UUID, user_id: uuid.UUID, entry_data: EntryUpdate) -> Entry:
        """Update an entry."""
        entry = self._get_owned_entry(entry_id, user_id)

        # Handle journal change if requested
        old_journal_id = None
        new_journal_id = None
        if entry_data.journal_id is not None and entry_data.journal_id != entry.journal_id:
            # Validate new journal exists and belongs to user
            new_journal_statement = select(Journal).where(
                Journal.id == entry_data.journal_id,
                Journal.user_id == user_id
            )
            new_journal = self.session.exec(new_journal_statement).first()
            if not new_journal:
                log_warning(f"Target journal not found for user {user_id}: {entry_data.journal_id}")
                raise JournalNotFoundError("Target journal not found")

            # Prevent moving into archived journals
            if new_journal.is_archived:
                log_warning(f"Cannot move entry {entry_id} to archived journal {new_journal.id}")
                raise ValidationError("Cannot move entry to an archived journal")

            # Store old and new journal IDs for stats recalculation
            old_journal_id = entry.journal_id
            new_journal_id = entry_data.journal_id
            entry.journal_id = new_journal_id
            log_info(f"Entry {entry_id} journal changed from {old_journal_id} to {new_journal_id}")

        # Update fields
        if entry_data.title is not None:
            entry.title = entry_data.title
        if entry_data.content_delta is not None:
            from app.core.media_signing import normalize_delta_media_ids
            from app.models.entry import EntryMedia

            delta_payload = entry_data.content_delta.model_dump()
            sources = extract_media_sources(delta_payload)
            log_debug(
                "Entry update: incoming delta media sources",
                entry_id=str(entry.id),
                user_id=user_id,
                media_source_count=len(sources),
                redacted_media_ids=[f"{s[:8]}..." for s in sources[:5]],
            )
            media_items = self.session.exec(
                select(EntryMedia).where(EntryMedia.entry_id == entry.id)
            ).all()
            log_debug(
                "Entry update: existing media items",
                entry_id=str(entry.id),
                user_id=user_id,
                media_count=len(media_items),
                media_ids=[str(media.id) for media in media_items[:5]],
                immich_asset_count=len(
                    [
                        media
                        for media in media_items
                        if media.external_provider == IntegrationProvider.IMMICH.value
                        and media.external_asset_id
                    ]
                ),
            )
            normalized_delta = normalize_delta_media_ids(delta_payload, list(media_items))
            normalized_sources = extract_media_sources(normalized_delta)
            log_debug(
                "Entry update: normalized delta media sources",
                entry_id=str(entry.id),
                user_id=user_id,
                media_source_count=len(normalized_sources),
                redacted_media_ids=[f"{s[:8]}..." for s in normalized_sources[:5]],
            )
            entry.content_delta = normalized_delta
            plain_text = extract_plain_text(normalized_delta)
            entry.content_plain_text = plain_text or None
            entry.word_count = len(plain_text.split()) if plain_text else 0
        if entry_data.entry_timezone is not None:
            tz_value = (entry_data.entry_timezone or "UTC").strip() or "UTC"
            entry.entry_timezone = tz_value
        timestamp_changed = False

        if entry_data.entry_datetime_utc is not None:
            entry.entry_datetime_utc = ensure_utc(entry_data.entry_datetime_utc)
            timestamp_changed = True

        if entry_data.entry_date is not None:
            timezone_name = entry.entry_timezone or "UTC"
            base_dt = ensure_utc(entry.entry_datetime_utc)
            local_current = base_dt.astimezone(ZoneInfo(timezone_name))
            target_local = datetime.combine(entry_data.entry_date, local_current.time())
            entry.entry_datetime_utc = to_utc(target_local, timezone_name)
            timestamp_changed = True

        if timestamp_changed or entry_data.entry_timezone is not None:
            self._refresh_entry_date(entry)

        # Update structured location/weather fields
        if entry_data.location_json is not None:
            entry.location_json = entry_data.location_json
        if entry_data.latitude is not None:
            entry.latitude = entry_data.latitude
        if entry_data.longitude is not None:
            entry.longitude = entry_data.longitude
        if entry_data.weather_json is not None:
            entry.weather_json = entry_data.weather_json
        if entry_data.weather_summary is not None:
            entry.weather_summary = entry_data.weather_summary

        if entry_data.is_pinned is not None:
            entry.is_pinned = entry_data.is_pinned

        entry.updated_at = utc_now()
        try:
            self.session.add(entry)
            self._commit()
            self.session.refresh(entry)
        except SQLAlchemyError as exc:
            self.session.rollback()
            log_error(exc)
            raise

        # Recalculate stats for both journals if journal was changed
        if old_journal_id is not None and new_journal_id is not None:
            try:
                from app.services.journal_service import JournalService
                journal_service = JournalService(self.session)
                # Recalculate old journal stats
                journal_service.recalculate_journal_entry_count(old_journal_id, user_id)
                # Recalculate new journal stats
                journal_service.recalculate_journal_entry_count(new_journal_id, user_id)
            except JournalNotFoundError:
                log_warning(f"Journal missing during entry update recount for user {user_id}")
            except SQLAlchemyError as exc:
                log_error(exc)
            except Exception as exc:
                log_error(exc)

        log_info(f"Entry updated for user {user_id}: {entry.id}")
        return entry

    async def delete_entry(self, entry_id: uuid.UUID, user_id: uuid.UUID) -> bool:
        """Hard delete an entry and its related records."""
        entry = self._get_owned_entry(entry_id, user_id)

        # Hard delete related EntryMedia records
        from app.services.media_service import MediaService
        from app.services.media_storage_service import MediaStorageService

        media_statement = select(EntryMedia).where(EntryMedia.entry_id == entry_id)
        media_records = self.session.exec(media_statement).all()

        media_service = MediaService()
        # Note: We'll create storage service AFTER commit when reference counts are accurate
        media_files_to_delete = []

        # Collect linked assets and check if they're used in other entries BEFORE deletion
        # This is critical because once we delete the media records, we can't query for remaining references
        linked_asset_ids = [
            media.external_asset_id
            for media in media_records
            if media.external_provider == "immich" and not media.file_path and media.external_asset_id
        ]

        linked_assets_to_remove = []
        if linked_asset_ids:
            # Single query: count occurrences of each asset across all user entries (excluding current entry)
            from sqlalchemy import func
            count_statement = (
                select(EntryMedia.external_asset_id, func.count(EntryMedia.id).label('count'))
                .where(
                    EntryMedia.external_asset_id.in_(linked_asset_ids),
                    EntryMedia.external_provider == "immich",
                    EntryMedia.entry_id != entry_id
                )
                .join(Entry)
                .where(Entry.user_id == user_id)
                .group_by(EntryMedia.external_asset_id)
            )

            # Get assets that are used in other entries
            asset_counts = self.session.exec(count_statement).all()
            assets_in_use = {asset_id for asset_id, count in asset_counts if count > 0}

            # Only remove assets that are NOT in use elsewhere
            linked_assets_to_remove = [aid for aid in linked_asset_ids if aid not in assets_in_use]

        for media in media_records:
            # Collect media info for reference-counted deletion BEFORE deleting from DB
            if media.file_path:
                media_files_to_delete.append({
                    'file_path': media.file_path,
                    'checksum': media.checksum,  # May be None for older records
                    'thumbnail_path': media.thumbnail_path,
                    'force': media.checksum is None  # Force delete if no checksum
                })

            self.session.delete(media)

        # Hard delete related EntryTagLink records
        tag_link_statement = select(EntryTagLink).where(EntryTagLink.entry_id == entry_id)
        tag_link_records = self.session.exec(tag_link_statement).all()
        for tag_link in tag_link_records:
            self.session.delete(tag_link)

        # Store journal_id for recount before deleting entry
        journal_id = entry.journal_id

        # Hard delete the entry
        self.session.delete(entry)

        try:
            self._commit()
        except SQLAlchemyError as exc:
            self.session.rollback()
            log_error(exc)
            raise

        # Trigger removal from Immich album for linked assets (only those not used elsewhere)
        # This must happen AFTER commit to ensure the background task sees the committed state
        if linked_assets_to_remove:
            try:
                from app.core.celery_app import celery_app
                celery_app.send_task(
                    "app.integrations.tasks.remove_assets_from_album_task",
                    args=[str(user_id), "immich", linked_assets_to_remove]
                )
            except Exception as exc:
                log_warning(f"Failed to trigger album asset removal task: {exc}")

        try:
            from app.services.journal_service import JournalService
            JournalService(self.session).recalculate_journal_entry_count(journal_id, user_id)
        except JournalNotFoundError:
            log_warning(f"Journal missing during entry delete recount for user {user_id}: {journal_id}")
        except SQLAlchemyError as exc:
            log_error(exc)
        except Exception as exc:
            log_error(exc)

        # Recalculate writing streak statistics after entry is deleted
        # This ensures analytics reflect the correct entry counts
        try:
            from app.services.analytics_service import AnalyticsService
            analytics_service = AnalyticsService(self.session)
            analytics_service.recalculate_writing_streak_stats(user_id)
        except Exception as exc:
            # Log error but don't fail the deletion
            log_warning(f"Failed to update writing streak stats after entry deletion: {exc}")

        # Delete physical media files from disk using reference counting
        # Create storage service with fresh session AFTER commit to get accurate reference counts
        from app.core.database import get_session_context

        for media_info in media_files_to_delete:
            try:
                # Create a new session for reference counting (after DB records are deleted)
                with get_session_context() as fresh_session:
                    media_storage_service = MediaStorageService(media_service.media_root, fresh_session)
                    # Delete main file with reference counting
                    # Force delete if no checksum (can't do reference counting without checksum)
                    media_storage_service.delete_media(
                        relative_path=media_info['file_path'],
                        checksum=media_info['checksum'],
                        user_id=str(user_id),
                        force=media_info.get('force', False)
                    )

                # Delete thumbnail if it exists (thumbnails are not deduplicated)
                if media_info['thumbnail_path']:
                    thumbnail_full_path = (media_service.media_root / media_info['thumbnail_path']).resolve()
                    if thumbnail_full_path.exists() and str(thumbnail_full_path).startswith(str(media_service.media_root.resolve())):
                        thumbnail_full_path.unlink(missing_ok=True)
            except Exception as exc:
                log_warning(f"Failed to delete media file {media_info['file_path']} after entry deletion: {exc}")

        log_info(f"Entry hard-deleted for user {user_id}: {entry_id}")
        return True

    def toggle_pin(self, entry_id: uuid.UUID, user_id: uuid.UUID) -> Entry:
        """Toggle pin status of an entry."""
        entry = self._get_owned_entry(entry_id, user_id)

        entry.is_pinned = not entry.is_pinned
        entry.updated_at = utc_now()
        try:
            self.session.add(entry)
            self._commit()
            self.session.refresh(entry)
        except SQLAlchemyError as exc:
            self.session.rollback()
            log_error(exc)
            raise

        log_info(f"Entry pin toggled for user {user_id}: {entry.id} -> {entry.is_pinned}")
        return entry

    def search_entries(
        self,
        user_id: uuid.UUID,
        query: str,
        journal_id: Optional[uuid.UUID] = None,
        limit: int = DEFAULT_ENTRY_PAGE_LIMIT,
        offset: int = 0,
        include_drafts: bool = False,
        hydrate_media: bool = True,
    ) -> List[Entry]:
        """Search entries by content."""
        statement = select(Entry).where(
            Entry.user_id == user_id,
            Entry.content_plain_text.ilike(f"%{self._escape_like_pattern(query)}%", escape="\\")
        )

        if not include_drafts:
            statement = statement.where(Entry.is_draft.is_(False))

        if journal_id:
            statement = statement.where(Entry.journal_id == journal_id)

        statement = statement.order_by(Entry.entry_datetime_utc.desc()).offset(offset).limit(limit)
        entries = list(self.session.exec(statement))
        if hydrate_media:
            return [self._hydrate_entry(entry, user_id) for entry in entries]
        return entries

    def get_entries_by_date_range(
        self,
        user_id: uuid.UUID,
        start_date: date,
        end_date: date,
        journal_id: Optional[uuid.UUID] = None,
        include_drafts: bool = False,
        hydrate_media: bool = True,
    ) -> List[Entry]:
        """Get entries within a date range based on entry_date."""
        statement = select(Entry).where(
            Entry.user_id == user_id,
            Entry.entry_date >= start_date,
            Entry.entry_date <= end_date
        )

        if not include_drafts:
            statement = statement.where(Entry.is_draft.is_(False))

        if journal_id:
            statement = statement.where(Entry.journal_id == journal_id)

        statement = statement.order_by(Entry.entry_datetime_utc.desc())
        entries = list(self.session.exec(statement))
        if hydrate_media:
            return [self._hydrate_entry(entry, user_id) for entry in entries]
        return entries

    def _hydrate_entry(self, entry: Entry, user_id: uuid.UUID) -> Entry:
        """
        Hydrate media UUIDs in content_delta to signed URLs.

        Optimization: Skip traversal if media_count == 0.
        """
        if entry.media_count == 0 or not entry.content_delta:
            return entry

        from app.models.entry import EntryMedia
        from app.models.integration import Integration, IntegrationProvider
        from app.core.media_signing import attach_signed_urls_to_delta
        from app.schemas.entry import QuillDelta

        media = self.session.exec(
            select(EntryMedia).where(EntryMedia.entry_id == entry.id)
        ).all()

        immich_integration = self.session.exec(
            select(Integration)
            .where(Integration.user_id == user_id)
            .where(Integration.provider == IntegrationProvider.IMMICH)
        ).first()
        immich_base_url = immich_integration.base_url if immich_integration else None

        delta_dict = attach_signed_urls_to_delta(
            entry.content_delta,
            list(media),
            str(user_id),
            external_base_url=immich_base_url,
        )

        if delta_dict:
            entry.content_delta = QuillDelta.model_validate(delta_dict).model_dump()

        return entry

    def add_media_to_entry(
        self,
        entry_id: uuid.UUID,
        user_id: uuid.UUID,
        media_data: EntryMediaCreate | EntryMediaCreateRequest
    ) -> EntryMedia:
        """Add media to an entry."""
        # Verify the entry belongs to the user
        self._get_owned_entry(entry_id, user_id)

        media = EntryMedia(
            entry_id=entry_id,
            media_type=media_data.media_type,
            file_path=media_data.file_path,
            original_filename=media_data.original_filename,
            file_size=media_data.file_size,
            mime_type=media_data.mime_type,
            thumbnail_path=media_data.thumbnail_path,
            duration=media_data.duration,
            width=media_data.width,
            height=media_data.height,
            alt_text=media_data.alt_text,
            upload_status=media_data.upload_status,
            file_metadata=media_data.file_metadata,
            checksum=media_data.checksum,
            external_provider=getattr(media_data, "external_provider", None),
            external_asset_id=getattr(media_data, "external_asset_id", None),
            external_url=getattr(media_data, "external_url", None),
            external_created_at=getattr(media_data, "external_created_at", None),
            external_metadata=getattr(media_data, "external_metadata", None),
        )

        try:
            self.session.add(media)
            self._commit()
            self.session.refresh(media)
        except SQLAlchemyError as exc:
            self.session.rollback()
            log_error(exc)
            raise

        log_info(f"Media added to entry {entry_id} for user {user_id}: {media.id}")

        # Trigger addition to Immich album for linked assets
        if media.external_provider == "immich" and not media.file_path and media.external_asset_id:
            try:
                from app.core.celery_app import celery_app
                celery_app.send_task(
                    "app.integrations.tasks.add_assets_to_album_task",
                    args=[str(user_id), "immich", [media.external_asset_id]]
                )
            except Exception as exc:
                log_warning(f"Failed to trigger album asset addition task: {exc}")

        return media

    def get_entry_media(self, entry_id: uuid.UUID, user_id: uuid.UUID) -> List[EntryMedia]:
        """Get all media for an entry."""
        # Verify the entry belongs to the user
        self._get_owned_entry(entry_id, user_id)

        statement = select(EntryMedia).where(
            EntryMedia.entry_id == entry_id,
        )
        return list(self.session.exec(statement))

    def delete_entry_media(self, media_id: uuid.UUID, user_id: uuid.UUID) -> bool:
        """Hard delete an entry media file.

        Args:
            media_id: Media ID to delete
            user_id: User ID for authorization

        Returns:
            True if deleted successfully

        Raises:
            EntryNotFoundError: If media doesn't exist or doesn't belong to user's entry
        """
        # Get the media and verify it belongs to user's entry
        statement = select(EntryMedia).join(Entry).where(
            EntryMedia.id == media_id,
            Entry.user_id == user_id,
        )
        media = self.session.exec(statement).first()

        if not media:
            raise EntryNotFoundError("Media not found")

        # Check if this asset is used in other entries BEFORE deletion
        should_remove_from_album = False
        if media.external_provider == "immich" and not media.file_path and media.external_asset_id:
            # Count how many other entries use this asset (excluding current media record)
            from sqlalchemy import func
            count_statement = (
                select(func.count(EntryMedia.id))
                .where(
                    EntryMedia.external_asset_id == media.external_asset_id,
                    EntryMedia.external_provider == "immich",
                    EntryMedia.id != media_id
                )
                .join(Entry)
                .where(Entry.user_id == user_id)
            )

            other_usage_count = self.session.exec(count_statement).one()

            # Only mark for removal if no other entries use this asset
            should_remove_from_album = (other_usage_count == 0)

        # Store asset info for album removal after commit
        asset_id_to_remove = media.external_asset_id if should_remove_from_album else None

        # Hard delete the media
        self.session.delete(media)
        try:
            self._commit()
        except SQLAlchemyError as exc:
            self.session.rollback()
            log_error(exc)
            raise

        # Trigger removal from Immich album after successful deletion
        if asset_id_to_remove:
            try:
                from app.core.celery_app import celery_app
                celery_app.send_task(
                    "app.integrations.tasks.remove_assets_from_album_task",
                    args=[str(user_id), "immich", [asset_id_to_remove]]
                )
            except Exception as exc:
                log_warning(f"Failed to trigger album asset removal task: {exc}")

        log_info(f"Media hard-deleted for user {user_id}: {media.id}")
        return True
