"""
Service for managing Immich import jobs.
Handles both link-only and copy mode imports.
"""
import asyncio
import time
import shutil
import tempfile
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

from immich.client.generated.models.asset_media_size import AssetMediaSize
from sqlmodel import Session, select

from app.core.encryption import decrypt_token
from app.models.import_job import ImportJob
from app.models.enums import JobStatus, ImportSourceType, MediaType, UploadStatus
from app.models.user import User
from app.models.entry import Entry, EntryMedia
from app.models.integration import Integration, IntegrationProvider, ImportMode
from app.schemas.entry import EntryMediaCreate
from app.services.entry_service import EntryService
from app.services.media_service import MediaService
from app.integrations import immich_ as immich

from app.core.logging_config import log_info, log_error, log_warning
from app.core.media_signing import normalize_delta_media_ids
from app.core.scoped_cache import ScopedCache
from app.utils.quill_delta import extract_plain_text

# Batch size for parallel downloads (configurable in future)
COPY_MODE_BATCH_SIZE = 3

# Media type mapping from Immich asset types
IMMICH_TYPE_TO_MEDIA_TYPE = {
    "IMAGE": MediaType.IMAGE,
    "VIDEO": MediaType.VIDEO,
    "AUDIO": MediaType.AUDIO,
}

# Shared cache instance
_normalize_cache: Optional[ScopedCache] = None


def _map_immich_type_to_media_type(immich_type: str) -> MediaType:
    """Map Immich asset type to MediaType enum."""
    return IMMICH_TYPE_TO_MEDIA_TYPE.get(immich_type, MediaType.UNKNOWN)


class ImportJobService:
    """Service for managing Immich import jobs (supports both link-only and copy modes)."""

    def __init__(self, session: Session):
        global _normalize_cache
        self.session = session
        self.entry_service = EntryService(session)
        self.media_service = MediaService(session)
        self._immich_provider = "immich"
        if _normalize_cache is None:
            _normalize_cache = ScopedCache("immich_entry_normalize")
        self._normalize_cache = _normalize_cache

    def _maybe_normalize_entry_delta(
        self,
        entry_id: Optional[uuid.UUID],
        session: Session,
        *,
        debounce_seconds: int = 2,
        commit: bool = False,
    ) -> None:
        """
        Normalize entry delta if needed (syncs media IDs between delta and EntryMedia).

        Args:
            entry_id: The ID of the entry to normalize.
            session: The database session to use.
            debounce_seconds: Minimum time in seconds between normalizations for same entry.
            commit: Whether to commit the session after adding changes.
                    Default is False to allow batching transactions.
        """
        if entry_id is None:
            return
        cache_key = str(entry_id)
        cached = self._normalize_cache.get(cache_key, "debounce") or {}
        last_ts = cached.get("ts")
        now = time.time()
        if isinstance(last_ts, (int, float)) and now - last_ts < debounce_seconds:
            return

        self._normalize_cache.set(cache_key, "debounce", {"ts": now}, ttl_seconds=debounce_seconds)

        entry = session.get(Entry, entry_id)
        if not entry or not entry.content_delta:
            return

        media_items = session.exec(
            select(EntryMedia).where(EntryMedia.entry_id == entry_id)
        ).all()
        normalized = normalize_delta_media_ids(entry.content_delta, list(media_items))
        if normalized != entry.content_delta:
            entry.content_delta = normalized
            plain_text = extract_plain_text(normalized)
            entry.content_plain_text = plain_text or None
            entry.word_count = len(plain_text.split()) if plain_text else 0
            session.add(entry)
            if commit:
                session.commit()

    def _get_existing_external_media(
        self,
        entry_id: uuid.UUID,
        asset_id: str,
        session: Optional[Session] = None
    ) -> Optional[EntryMedia]:
        active_session = session or self.session
        return active_session.exec(
            select(EntryMedia)
            .where(EntryMedia.entry_id == entry_id)
            .where(EntryMedia.external_provider == self._immich_provider)
            .where(EntryMedia.external_asset_id == asset_id)
        ).first()

    def _resolve_media_type_from_asset(self, asset_type: Optional[str]) -> MediaType:
        return _map_immich_type_to_media_type(asset_type or "OTHER")

    def _extract_immich_metadata(self, asset_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract and normalize metadata from Immich asset data.
        Returns a dictionary with normalized fields.
        """
        exif_info = asset_data.get("exifInfo") or {}

        # Determine media type
        immich_type = asset_data.get("type", "OTHER")
        media_type = _map_immich_type_to_media_type(immich_type)

        # Parse taken_at
        taken_at_str = exif_info.get("dateTimeOriginal") or asset_data.get("createdAt")
        taken_at = None
        if taken_at_str:
            try:
                taken_at = datetime.fromisoformat(taken_at_str.replace("Z", "+00:00"))
            except (ValueError, AttributeError) as e:
                log_warning(f"Failed to parse date: {e}")

        # Extract dimensions and duration
        width = exif_info.get("exifImageWidth") or asset_data.get("width")
        height = exif_info.get("exifImageHeight") or asset_data.get("height")
        duration = self._parse_duration_seconds(asset_data.get("duration"))
        file_size = exif_info.get("fileSizeInByte") or asset_data.get("fileSizeInByte")

        # Get filename and mime type
        filename = (
            asset_data.get("originalFileName") or
            asset_data.get("originalPath") or
            f"Asset {str(asset_data.get('id', ''))[:8]}"
        )
        mime_type = asset_data.get("mimeType") or "application/octet-stream"

        # Build external metadata
        external_metadata = {
            "type": immich_type,
            "mimeType": mime_type,
            "thumbUrl": asset_data.get("thumbUrl") or f"/api/v1/integrations/immich/proxy/{asset_data.get('id')}/thumbnail",
            "width": width,
            "height": height,
            "exif": exif_info
        }

        return {
            "media_type": media_type,
            "taken_at": taken_at,
            "width": width,
            "height": height,
            "duration": duration,
            "file_size": file_size,
            "original_filename": filename,
            "mime_type": mime_type,
            "external_metadata": external_metadata
        }

    def _upsert_entry_media(
        self,
        entry_id: uuid.UUID,
        user_id: uuid.UUID,
        asset_id: str,
        asset_data: Optional[Dict[str, Any]] = None,
        file_info: Optional[Dict[str, Any]] = None,
        upload_status: UploadStatus = UploadStatus.PROCESSING,
        session: Optional[Session] = None,
        commit: bool = True
    ) -> EntryMedia:
        """
        Create or update EntryMedia for an external asset.
        """
        active_session = session or self.session
        existing = self._get_existing_external_media(entry_id, asset_id, session=active_session)

        # Extract normalized metadata if asset_data is provided
        metadata = self._extract_immich_metadata(asset_data) if asset_data else {}

        # Combine if we have file_info from local save
        file_path = (file_info or {}).get("file_path")
        file_size = (file_info or {}).get("file_size") or metadata.get("file_size")
        checksum = (file_info or {}).get("checksum")
        thumbnail_path = (file_info or {}).get("thumbnail_path")

        if existing:
            # Update existing record
            if metadata.get("media_type") and existing.media_type == MediaType.UNKNOWN:
                existing.media_type = metadata["media_type"]

            if metadata.get("original_filename"):
                existing.original_filename = metadata["original_filename"]

            if metadata.get("taken_at"):
                existing.external_created_at = metadata["taken_at"]

            if metadata.get("mime_type"):
                existing.mime_type = metadata["mime_type"]

            if metadata.get("external_metadata"):
                if not existing.external_metadata:
                    existing.external_metadata = metadata["external_metadata"]
                else:
                    existing.external_metadata = {**existing.external_metadata, **metadata["external_metadata"]}

            if file_path: 
                existing.file_path = file_path
            if file_size: 
                existing.file_size = file_size
            if checksum: 
                existing.checksum = checksum
            if thumbnail_path: 
                existing.thumbnail_path = thumbnail_path

            existing.upload_status = upload_status

            active_session.add(existing)
            if commit:
                active_session.commit()
            active_session.refresh(existing)
            return existing

        # Create new record
        media_create = EntryMediaCreate(
            entry_id=entry_id,
            media_type=metadata.get("media_type", MediaType.UNKNOWN),
            file_path=file_path,
            file_size=file_size,
            original_filename=metadata.get("original_filename") or f"Immich asset {asset_id[:8]}",
            mime_type=metadata.get("mime_type") or "application/octet-stream",
            thumbnail_path=thumbnail_path,
            checksum=checksum,
            duration=metadata.get("duration"),
            width=metadata.get("width"),
            height=metadata.get("height"),
            alt_text=f"Immich asset: {metadata.get('original_filename', asset_id)}",
            upload_status=upload_status,
            external_provider=self._immich_provider,
            external_asset_id=asset_id,
            external_created_at=metadata.get("taken_at"),
            external_metadata=metadata.get("external_metadata", {}),
        )
        media = self.entry_service.add_media_to_entry(entry_id, user_id, media_create)
        return media

    def create_placeholder_media(
        self,
        entry_id: uuid.UUID,
        user_id: uuid.UUID,
        asset_id: str,
        integration: Integration,
        asset_payload: Optional[Any] = None,
        session: Optional[Session] = None
    ) -> EntryMedia:
        """
        Create a placeholder EntryMedia record for an Immich asset.
        """
        asset_data = None
        if asset_payload:
            # Handle both dict and object (Pydantic/etc)
            if hasattr(asset_payload, "dict"):
                asset_data = asset_payload.dict()
            elif isinstance(asset_payload, dict):
                asset_data = asset_payload
            else:
                # Fallback: try to extract common fields if it's an object
                asset_data = {
                    "id": asset_id,
                    "type": getattr(asset_payload, "type", None),
                    "title": getattr(asset_payload, "title", None),
                    "taken_at": getattr(asset_payload, "taken_at", None),
                    "thumbUrl": getattr(asset_payload, "thumb_url", None),
                    "originalUrl": getattr(asset_payload, "original_url", None),
                    "metadata": getattr(asset_payload, "metadata", None),
                }

        upload_status = (
            UploadStatus.COMPLETED
            if integration.import_mode == ImportMode.LINK_ONLY
            else UploadStatus.PROCESSING
        )

        return self._upsert_entry_media(
            entry_id=entry_id,
            user_id=user_id,
            asset_id=asset_id,
            asset_data=asset_data,
            upload_status=upload_status,
            session=session
        )

    def _mark_media_failed(
        self,
        entry_id: Optional[uuid.UUID],
        asset_id: str,
        session: Session,
        error_message: Optional[str] = None,
        commit: bool = True
    ) -> None:
        if entry_id is None:
            return
        media = self._get_existing_external_media(entry_id, asset_id, session=session)
        if not media:
            return
        media.upload_status = UploadStatus.FAILED
        if error_message:
            media.processing_error = error_message[:1000]
        session.add(media)
        if commit:
            session.commit()

    def create_job(
        self,
        user_id: uuid.UUID,
        entry_id: uuid.UUID,
        asset_ids: list[str]
    ) -> ImportJob:
        """
        Create a new import job record for Immich.
        """
        job = ImportJob(
            user_id=user_id,
            entry_id=entry_id,
            source_type=ImportSourceType.IMMICH,
            result_data={"asset_ids": asset_ids},
            total_items=len(asset_ids),
            status=JobStatus.PENDING
        )
        self.session.add(job)
        self.session.commit()
        self.session.refresh(job)

        log_info(
            f"Created Immich import job {job.id} for user {user_id}: "
            f"{len(asset_ids)} assets to entry {entry_id}"
        )

        return job

    def get_job(self, job_id: uuid.UUID, user_id: uuid.UUID) -> Optional[ImportJob]:
        """
        Get import job by ID and user ID.
        """
        return self.session.exec(
            select(ImportJob)
            .where(ImportJob.id == job_id)
            .where(ImportJob.user_id == user_id)
        ).first()



    def _create_link_only_media(
        self,
        entry_id: uuid.UUID,
        user_id: uuid.UUID,
        asset_id: str,
        asset_metadata: dict,
        integration: Integration,
        session: Optional[Session] = None,
        commit: bool = True
    ):
        """
        Create or update EntryMedia record for link-only Immich asset.
        """
        media = self._upsert_entry_media(
            entry_id=entry_id,
            user_id=user_id,
            asset_id=asset_id,
            asset_data=asset_metadata,
            upload_status=UploadStatus.COMPLETED,
            session=session,
            commit=commit
        )
        log_info(f"Updated link-only EntryMedia for Immich asset {asset_id}")
        return media

    async def create_and_process_job_async(
        self,
        user_id: uuid.UUID,
        entry_id: uuid.UUID,
        asset_ids: list[str],
        assets: Optional[list[Any]] = None
    ) -> ImportJob:
        """
        Create an import job for async processing (copy mode).

        Creates placeholder media records first, then the job.
        """
        from app.core.database import engine

        # Use a short-lived session for placeholder + job creation to avoid blocking on long-lived transactions from entry updates.
        thread_session = Session(engine)
        try:
            thread_service = ImportJobService(thread_session)

            # Fetch integration needed for placeholders
            integration = thread_session.exec(
                select(Integration)
                .where(Integration.user_id == user_id)
                .where(Integration.provider == IntegrationProvider.IMMICH)
            ).first()

            if not integration:
                raise ValueError("Immich integration not found")

            assets_by_id = {}
            if assets:
                assets_by_id = {
                    (asset.id if hasattr(asset, "id") else asset.get("id")): asset
                    for asset in assets
                }
            for asset_id in asset_ids:
                try:
                    thread_service.create_placeholder_media(
                        entry_id=entry_id,
                        user_id=user_id,
                        asset_id=asset_id,
                        integration=integration,
                        asset_payload=assets_by_id.get(asset_id),
                    )
                except Exception as e:
                    log_warning(f"Failed to create placeholder for {asset_id}: {e}")
                    # Continue - job will try to process anyway

            job = thread_service.create_job(
                user_id=user_id,
                entry_id=entry_id,
                asset_ids=asset_ids
            )
            return job
        finally:
            thread_session.close()

    async def process_copy_job_async(
        self,
        job_id: uuid.UUID
    ) -> None:
        """
        Background task to process copy-mode import job.

        Uses Immich SDK (async with client); one method per asset: download original + thumbnail via SDK, then save and upsert.
        """
        from app.core.database import engine

        thread_session = Session(engine)
        try:
            thread_service = ImportJobService(thread_session)
            job = thread_session.exec(
                select(ImportJob).where(ImportJob.id == job_id)
            ).first()

            if not job:
                log_error(f"Import job {job_id} not found")
                return

            job.mark_running()
            thread_session.add(job)
            thread_session.commit()

            log_info(
                f"Processing copy-mode import job {job_id}: {job.total_items} assets"
            )

            user = thread_session.get(User, job.user_id)
            if not user:
                raise ValueError(f"User {job.user_id} not found")

            integration = thread_session.exec(
                select(Integration)
                .where(Integration.user_id == job.user_id)
                .where(Integration.provider == IntegrationProvider.IMMICH)
            ).first()

            if not integration or not integration.is_active:
                raise ValueError("Immich integration not active")

            api_key = decrypt_token(integration.access_token_encrypted)
            base_url = integration.base_url.rstrip("/")
            asset_ids = job.result_data.get("asset_ids", [])

            async with immich._create_immich_client(
                api_key=api_key, base_url=base_url
            ) as client:
                for i in range(0, len(asset_ids), COPY_MODE_BATCH_SIZE):
                    batch = asset_ids[i : i + COPY_MODE_BATCH_SIZE]
                    tasks = [
                        thread_service._copy_one_asset_async(
                            client, uuid.UUID(aid), job, integration, thread_session
                        )
                        for aid in batch
                    ]
                    results = await asyncio.gather(*tasks, return_exceptions=True)

                    processed = 0
                    failed = 0
                    for aid, result in zip(batch, results):
                        if isinstance(result, Exception):
                            log_error(result)
                            thread_service._mark_media_failed(
                                job.entry_id, aid, thread_session, str(result)
                            )
                            failed += 1
                        elif result:
                            processed += 1
                        else:
                            thread_service._mark_media_failed(
                                job.entry_id, aid, thread_session
                            )
                            failed += 1

                    job.update_progress(
                        job.processed_items + processed,
                        job.total_items,
                        job.failed_items + failed,
                    )
                    thread_session.add(job)
                    thread_session.commit()

            if job.failed_items == 0 and job.status not in {
                JobStatus.FAILED,
                JobStatus.PARTIAL,
                JobStatus.CANCELLED,
            }:
                job.mark_completed()
            thread_session.add(job)
            thread_session.commit()

            log_info(
                f"Copy-mode import job {job_id} completed: "
                f"{job.processed_items} succeeded, {job.failed_items} failed"
            )

        except Exception as e:
            log_error(e)
            if "job" in locals():
                job.mark_failed(str(e)[:2000])
                thread_session.add(job)
                thread_session.commit()
        finally:
            thread_session.close()

    async def _copy_one_asset_async(
        self,
        client: immich.ImmichAsyncClient,
        asset_id_uuid: uuid.UUID,
        job: ImportJob,
        integration: Integration,
        session: Session,
    ) -> bool:
        """
        Download one asset (original + thumbnail) via Immich SDK, save to storage, upsert EntryMedia, post-process.
        """
        temp_dir = Path(tempfile.mkdtemp())
        try:
            try:
                asset = await client.assets.get_asset_info(id=asset_id_uuid)
                if not asset:
                    self._mark_media_failed(
                        job.entry_id, str(asset_id_uuid), session
                    )
                    return False
            except Exception as e:
                log_error(f"get_asset_info failed for {asset_id_uuid}: {e}")
                self._mark_media_failed(
                    job.entry_id, str(asset_id_uuid), session, str(e)
                )
                return False

            asset_dict = asset.model_dump(by_alias=True)
            metadata = self._extract_immich_metadata(asset_dict)
            filename = metadata["original_filename"]
            user_id = str(job.user_id)
            is_video = getattr(asset.type, "value", str(asset.type)) == "VIDEO"

            try:
                if is_video:
                    path_original = await client.assets.play_asset_video_to_file(
                        id=asset_id_uuid, out_dir=temp_dir, show_progress=False
                    )
                else:
                    path_original = await client.assets.download_asset_to_file(
                        id=asset_id_uuid, out_dir=temp_dir, show_progress=False
                    )

                path_thumb = await client.assets.view_asset_to_file(
                    id=asset_id_uuid,
                    out_dir=temp_dir,
                    size=AssetMediaSize.PREVIEW,
                    show_progress=False,
                )
            except Exception as e:
                log_error(f"SDK download failed for {asset_id_uuid}: {e}")
                self._mark_media_failed(
                    job.entry_id, str(asset_id_uuid), session, str(e)
                )
                return False

            try:
                saved_info = await self.media_service.save_uploaded_file(
                    original_filename=filename,
                    user_id=user_id,
                    media_type=metadata["media_type"],
                    file_path=str(path_original),
                )
            except Exception as e:
                log_error(f"save_uploaded_file failed for {asset_id_uuid}: {e}")
                self._mark_media_failed(
                    job.entry_id, str(asset_id_uuid), session, str(e)
                )
                return False

            stored_filename = Path(saved_info["file_path"]).name
            if metadata["media_type"] == MediaType.IMAGE:
                thumbnail_filename = f"thumb_{stored_filename}"
            elif metadata["media_type"] == MediaType.VIDEO:
                thumbnail_filename = f"thumb_{Path(stored_filename).stem}.jpg"
            else:
                thumbnail_filename = f"thumb_{stored_filename}"

            thumbnail_path_obj = self.media_service._get_thumbnail_path(
                thumbnail_filename, metadata["media_type"], user_id=user_id
            )
            if thumbnail_path_obj and path_thumb.exists():
                thumbnail_path_obj.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(path_thumb, thumbnail_path_obj)
                saved_info["thumbnail_path"] = str(
                    thumbnail_path_obj.relative_to(self.media_service.media_root)
                )
                log_info(f"Saved thumbnail for asset {asset_id_uuid}: {saved_info['thumbnail_path']}")

            media = self._upsert_entry_media(
                entry_id=job.entry_id,
                user_id=job.user_id,
                asset_id=str(asset_id_uuid),
                asset_data=asset_dict,
                file_info=saved_info,
                upload_status=UploadStatus.COMPLETED,
                session=session,
            )

            try:
                self.media_service.process_uploaded_file(
                    media_id=str(media.id),
                    file_path=saved_info["full_file_path"],
                    user_id=user_id,
                )
            except Exception as e:
                log_warning(f"Processing failed for asset {asset_id_uuid}: {e}")

            log_info(f"Successfully imported original for asset {asset_id_uuid}")
            return True
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)

    async def process_link_only_job_async(
        self,
        job_id: uuid.UUID
    ) -> None:
        """
        Background task to process link-only import jobs.

        Fetches metadata from Immich and updates placeholder EntryMedia records.
        """
        from app.core.database import engine

        thread_session = Session(engine)
        try:
            thread_service = ImportJobService(thread_session)
            job = thread_session.exec(
                select(ImportJob)
                .where(ImportJob.id == job_id)
            ).first()

            if not job:
                log_error(f"Import job {job_id} not found")
                return

            job.mark_running()
            thread_session.add(job)
            thread_session.commit()

            user = thread_session.get(User, job.user_id)
            if not user:
                raise ValueError(f"User {job.user_id} not found")

            integration = thread_session.exec(
                select(Integration)
                .where(Integration.user_id == job.user_id)
                .where(Integration.provider == IntegrationProvider.IMMICH)
            ).first()

            if not integration or not integration.is_active:
                raise ValueError("Immich integration not active")

            api_key = decrypt_token(integration.access_token_encrypted)
            base_url = integration.base_url.rstrip('/')
            asset_ids = job.result_data.get("asset_ids", [])

            processed = 0
            failed = 0
            failed_asset_ids = []

            for asset_id in asset_ids:
                try:
                    asset_metadata = await immich.get_asset_info(
                        base_url=base_url,
                        api_key=api_key,
                        asset_id=asset_id
                    )

                    if asset_metadata:
                        thread_service._create_link_only_media(
                            entry_id=job.entry_id,
                            user_id=job.user_id,
                            asset_id=asset_id,
                            asset_metadata=asset_metadata,
                            integration=integration,
                            session=thread_session,
                            commit=False
                        )
                        thread_service._maybe_normalize_entry_delta(job.entry_id, thread_session, commit=False)
                        processed += 1
                    else:
                        thread_service._mark_media_failed(job.entry_id, asset_id, thread_session, commit=False)
                        failed += 1
                        failed_asset_ids.append(asset_id)
                except Exception as e:
                    log_error(e)
                    thread_service._mark_media_failed(job.entry_id, asset_id, thread_session, str(e), commit=False)
                    failed += 1
                    failed_asset_ids.append(asset_id)

                job.update_progress(processed, len(asset_ids), failed)
                thread_session.add(job)
                thread_session.commit()

            if failed_asset_ids:
                job.result_data["failed_asset_ids"] = failed_asset_ids

            if job.failed_items == 0 and job.status not in {JobStatus.FAILED, JobStatus.PARTIAL, JobStatus.CANCELLED}:
                job.mark_completed()
            thread_session.add(job)
            thread_session.commit()

        except Exception as e:
            log_error(e)
            if 'job' in locals():
                job.mark_failed(str(e))
                thread_session.add(job)
                thread_session.commit()
        finally:
            thread_session.close()



    @staticmethod
    def _parse_duration_seconds(raw_duration: Optional[object]) -> Optional[float]:
        """Convert Immich duration values into float seconds."""
        if raw_duration is None:
            return None
        if isinstance(raw_duration, (int, float)):
            return float(raw_duration)
        if not isinstance(raw_duration, str):
            return None
        value = raw_duration.strip()
        if not value:
            return None
        if ":" not in value:
            try:
                return float(value)
            except ValueError:
                return None

        # Expected format: HH:MM:SS(.mmm)
        parts = value.split(":")
        if len(parts) != 3:
            return None
        try:
            hours = int(parts[0])
            minutes = int(parts[1])
            seconds = float(parts[2])
        except ValueError:
            return None
        return float(hours * 3600 + minutes * 60 + seconds)


    async def repair_thumbnails_async(
        self,
        user_id: uuid.UUID,
        asset_ids: list[str]
    ) -> None:
        """
        Background task to repair missing thumbnails for Immich media.

        Uses Immich SDK (async with client); view_asset_to_file to temp dir, copy to Journiv path, update EntryMedia.
        """
        from app.core.database import engine

        thread_session = Session(engine)
        try:
            user = thread_session.get(User, user_id)
            if not user:
                log_error(f"User {user_id} not found for thumbnail repair")
                return

            integration = thread_session.exec(
                select(Integration)
                .where(Integration.user_id == user_id)
                .where(Integration.provider == IntegrationProvider.IMMICH)
            ).first()

            if not integration or not integration.is_active:
                log_error(f"Immich integration not active for user {user_id}")
                return

            api_key = decrypt_token(integration.access_token_encrypted)
            base_url = integration.base_url.rstrip("/")

            repaired_count = 0
            failed_count = 0

            async with immich._create_immich_client(
                api_key=api_key, base_url=base_url
            ) as client:
                for i in range(0, len(asset_ids), COPY_MODE_BATCH_SIZE):
                    batch = asset_ids[i : i + COPY_MODE_BATCH_SIZE]
                    tasks = [
                        self._repair_single_thumbnail(
                            client, asset_id, user_id, thread_session
                        )
                        for asset_id in batch
                    ]
                    results = await asyncio.gather(*tasks, return_exceptions=True)

                    for asset_id, result in zip(batch, results):
                        if isinstance(result, Exception):
                            log_error(result)
                            failed_count += 1
                        elif result:
                            repaired_count += 1
                        else:
                            failed_count += 1

            log_info(
                f"Thumbnail repair completed for user {user_id}: "
                f"{repaired_count} repaired, {failed_count} failed"
            )

        except Exception as e:
            log_error(e)
        finally:
            thread_session.close()

    async def _repair_single_thumbnail(
        self,
        client: immich.ImmichAsyncClient,
        asset_id: str,
        user_id: uuid.UUID,
        session: Session,
    ) -> bool:
        """
        Repair thumbnail for a single asset using SDK view_asset_to_file; copy to Journiv path and update EntryMedia.
        """
        temp_dir = Path(tempfile.mkdtemp())
        try:
            media = session.exec(
                select(EntryMedia)
                .join(Entry, Entry.id == EntryMedia.entry_id)
                .where(Entry.user_id == user_id)
                .where(EntryMedia.external_asset_id == asset_id)
                .where(EntryMedia.external_provider == "immich")
            ).first()

            if not media:
                log_warning(f"EntryMedia not found for asset {asset_id}")
                return False

            path_thumb = await client.assets.view_asset_to_file(
                id=uuid.UUID(asset_id),
                out_dir=temp_dir,
                size=AssetMediaSize.PREVIEW,
                show_progress=False,
            )

            if not path_thumb.exists():
                log_warning(f"Failed to download thumbnail for asset {asset_id}")
                return False

            stored_filename = Path(media.file_path or "").name or f"asset-{asset_id}"
            if media.media_type == MediaType.IMAGE:
                thumbnail_filename = f"thumb_{stored_filename}"
            elif media.media_type == MediaType.VIDEO:
                thumbnail_filename = f"thumb_{Path(stored_filename).stem}.jpg"
            else:
                thumbnail_filename = f"thumb_{stored_filename}"

            thumbnail_path_obj = self.media_service._get_thumbnail_path(
                thumbnail_filename, media.media_type, user_id=str(user_id)
            )
            if not thumbnail_path_obj:
                log_warning(f"Cannot resolve thumbnail path for asset {asset_id}")
                return False

            thumbnail_path_obj.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(path_thumb, thumbnail_path_obj)
            media.thumbnail_path = str(
                thumbnail_path_obj.relative_to(self.media_service.media_root)
            )
            session.add(media)
            session.commit()
            log_info(f"Repaired thumbnail for asset {asset_id}")
            return True

        except Exception as e:
            log_error(e)
            return False
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)
